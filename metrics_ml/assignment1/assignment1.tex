\documentclass[reqno,12pt,notitlepage]{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{pdflscape}
\usepackage{url}
\usepackage{caption}
\usepackage{multirow,rotating,multicol}
\usepackage{listings}
\usepackage[bookmarks,hidelinks]{hyperref}
\usepackage{longtable}
\usepackage{threeparttable}
%\usepackage[position=top, font=normalsize]{subfig}
\usepackage{subcaption}
\usepackage{caption}
\usepackage{float}
\usepackage{pgfpages,tikz}
\usepackage{adjustbox}


% modifications
\newtheorem{proposition}{Proposition}
\DeclareMathOperator*{\plim}{plim}

\newcommand{\question}[1]{ \begin{center} \noindent\colorbox{gray!8}{
\parbox{0.8\textwidth}{\vspace{0.125in} #1 \vspace{0.125in} } } \end{center} }

\newcommand{\EE}{\mathbb{E}}
\newcommand{\PP}{\mathbb{P}}

\newcommand{\var}[1]{\operatorname{Var}\left( #1 \right)}
\newcommand{\cov}[1]{\operatorname{Cov}\left( #1 \right)}


% hyperref settings
\hypersetup{%
   colorlinks=false
}

% geometry
\usepackage[left=1in, right=1in, top=1in, bottom=1in]{geometry}

% bib
\usepackage[round]{natbib}


% paragraphs
\setlength{\parindent}{0pt}
\setlength{\parskip}{12pt}

\usepackage{color}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true, % make the links colored
    linkcolor=blue, % color TOC links in blue
    citecolor=blue, %citation colors blue
    urlcolor=red, % color URLs in red
    linktoc=all % 'all' will create links for everything in the TOC
}


\title{Topics in Econometrics - Assignment 1}
\author{Yixin Sun}
\date{\today}

\begin{document}

\maketitle

\section*{Question 1b and 1c}
We see here that the average $\hat{\beta}_{1}$ is about the same as the true $\beta_1$. The variance of the estimate decreases from $p =1$ to $p = 5$, and then increases after that. This makes sense as the $p=1$ regression suffers from omitted variable bias, whereas $p >5$ is overfitting with variables that are not actually related to the outcome variable or explanatory variable of interest. Even if you included the regressors that are independent of the one you care about, these independent regressors interfere. 

\begin{table}[H]
    \centering
    \caption{Average and Variance of $\hat{\beta}_1$}
    \input{part_c}
\end{table}


\section*{Question 1d}
\begin{table}[H]
    \centering
    \caption{Frisch-Waugh-Lovell Results}
    \input{part_d}
\end{table}
This table shows that the value of the lowest eigenvalue decreases as the number of regressor increase, while $1 / N \sum_{i} \hat{X}_{i 1}(\tilde{p})^{2}$ increases as we add more regressors. $1 / N \sum_{i} \hat{X}_{i 1}(\tilde{p})^{2}$ here is the variance of $\hat{X}_{i 1}$. 


We're thinking about what this $\tilde{p-1}$ unrelated regressors are doing to the estimation of $\beta_1$, the fitted value of th first regressor we care about. In the DGP, they are all independent regressors, so $\hat{X}_{1}$ should be zero. But as you add more independent regressors, the estimate gets worse, and hence the variance increases. The lowest eigenvalue addresses the invertibility of the regressors - the lowest eigenvealue is closer to the zero and there is more noise in the inverse. This is the same thing as variance of $\hat{X}_{i 1}$ getting bigger, which we see in the second column of table 2. 


\section*{Question 1e}
This is even worse than b, c, d. As $\rho$ and $N$ increase, the variance of $\hat{\beta}_1$ increases. The issue here is that the extra covariates are not actually correlated to $Y_1$. However, the correlation between the extra covariates and $X_1$ is causing the extra covariates to essentially steal variation that should be attributed to $X_1$, leading to more noise on the parameter of interest, $\hat{\beta}_1$.
\begin{table}[H]
    \centering
    \caption{Dependence Structure Between Covariates}
    \input{part_e}
\end{table}

\section*{Question 1f}
When $\rho = 0$, there is no relationship between the covariates, and the eigenvalues decrease smoothly, since each additional coviariate adds unexplained variation to the set. However, when we do have correlation between covariates, we see that the eigenvalue starts off very high, and then immediately drops off. This shows the problem of collinearity: even if we have all these extra covariates, the strong correlation between them means that many do not hold any additional information.

\begin{figure}[H]
    \centering
    \includegraphics[width=.6\textwidth]{part_f.png}
\end{figure}

\newpage
\section*{Question 1g}

We see a similar relationship between $\rho$, $N$ and the variance of $\hat{\beta}_1$, but we see what happens with two different rates of convergence of $\frac{p}{N}$. When $p=0.9N$, $\lim_{N\rightarrow \infty} \frac{p}{N} = 0.9$. Thus we see in Table 4 that when $N$ increases, we do not see a huge impact on the lowest eigenvalue. When $p=20\times log(N)$, $\lim_{N\rightarrow \infty} \frac{p}{N} = 0$. We see that in Table 5, the eigenvalues increase dramatically as $N$ increases, and this translates to a lower variance on $\hat{\beta}_1$.


\begin{table}[H]
    \centering
    \caption{$p=0.9N$}
    \input{part_g1}
\end{table}


\begin{table}[H]
    \centering
    \caption{$p=20\times log(N)$}
    \input{part_g2}
\end{table}


\end{document}
